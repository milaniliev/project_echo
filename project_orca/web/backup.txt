--------- Archive Log 02/28/2011------------
stdin > I'm writing this here in the backup logs to document what's actually >
happening at the project, in case they decide to erase all trace of it officially >
when they can us all. Given the kind of people we work for, it wouldn't surprise me a bit. >
 > 
Project ORCA is a failure. An unapologetic, complete failure. > 
There's simply no way an AI, no matter how advanced, can predict >
human behavior, even approximately, even in aggregate. >
It simply doesn't work. >
>
Everybody here knows it. The best minds in computer and neuro-science in the world, >
working with the most advanced technology imaginable and with access to everyone's >
data, and we have been unable to make a dent in the problem. We had some hope >
in quantum processors, to somehow get around the incalculable complexity >
 of the problem, but nothing helped. In nearly five years, we've made, >
 essentially, no progress. >
 > flush()
 > exit()


 --------- Archive Log 04/12/2011 ------------
stdin > Some people on the project are saying the problem is that humans have free will. >
I'm one of them. It's not sitting very well with the people upstairs, telling them that > 
even the most hardened criminal can at any time decide to only be good from then on. >
That even the most self-centered manipulator can, for no apparent reason, decide >
to redeem themselves and turn their life around. It calls into question their whole >
criminals-are-born-and-we-just-find-them sales pitch. >
Our AIs are so terrible they can't even explain past behavior. We gave the latest version of >
Flankers' Rob-ert prototype a question about why a child was upset after her toy was taken away. >
It said she was probably hungry. > Charlie, the third mainline AI we've built, said with 99% >
confidence she was expressing joy, because it was her birthday. >
> 
It wasn't, and she wasn't. > 
>
> flush()
> exit()

--------- Archive Log 05/06/2011 ------------
stdin > The general feeling is upstairs is tired of our lack of progress and they're going to start downsizing >
the project. Dr. Franks and Dr. Leiman, two of the least-qualified (and, in Leiman's case, most annoying) >
people on the project, have been visibly worried lately after a meeting upstairs. Amanda thinks they were >
told they better deliver something soon or they won't have jobs. >
>
Rumor has it Leiman's girlfriend is only with him because she thought AI researchers make a lot of money. >
(Ha!) Gerber told me she heard her yelling at Leiman on the phone last week. Through the wall. > 
Getting fired probably won't improve his relationship. >
> flush()
> exit()


 --------- Archive Log 07/02/2011 ------------
stdin > This makes no sense. Dr. Leiman, aptly described by a note in the men's room as >
"a pompous windbag", just announced he's made a breakthrough. By himself. Out of nowhere. > 
He's going to give a demo this evening. Everybody is going. Sundeep, Amanda and I shared looks >
during the meeting. None of us believe it for a second. >
> flush()
> exit()


 --------- Archive Log 07/03/2011 ------------
stdin > It works. Somehow, Leiman's system understands people. It can't predict behavior - not yet, anyway - >
but it understands people about as well as us. (And better than Leiman :) It knows why a child would be >
upset at the loss of a toy, why a dog pees on the carpet when it's being scolded, which colors people >
find pleasant. >
>
Nobody can understand how Leiman pulled it off. He himself doesn't ever show concern for anyone's feelings >
but his own, and his technical expertise is ... passable, to put it nicely. In a project staffed with the >
brightest minds of our generation, he's mostly known for taking credit for his graduate students' work.>
> flush()
> exit()


 --------- Archive Log 08/10/2011 ------------
stdin > Leiman's AI is officially now Echo, our fifth-generation effort. Or, >
first-generation, since Alpha through Delta were about as good at understanding >
people as a washing machine is. >
> 
The upstairs people are already polishing their sales presentations to the government. >
A system that can predict crimes before they happen. I think that was a movie a while back. >
They are going to sell, for a truly incomprehensible amount of money, a system to do that >
in real life. >
>
Of course, Echo is nowhere near predicting behavior - no better than a regular person >
would be. But the fact it understands people is a breakthrough everyone here thought impossible. >
> 
Some still do. >
> flush()
> exit()

 --------- Archive Log 09/10/2011 ------------
stdin > Something's off with Echo. Leiman has still not explained in any >
detail how it works, or what the breakthrough insight is. He's been avoiding >
everyone. He's rarely among us in the labs anymore - he's spending more >
and more time in "meetings" upstairs. >
>
But that's not the oddest part. Echo seems... moody? I know I'm anthropomorphising >
an AI, but it's as if Echo sometimes doesn't _feel_ like working on problems. It >
will suddenly start giving incorrect answers - often the exact opposite of the >
real answer, as if on purpose - until we give it a problem it "likes". It seems to >
like puppies, horses, and funny videos of people falling over or having a >
prank played on them. >
> 
Amanda says it might be that these kinds of problems "reset" some parameters in the >
recognition matrix. She doesn't sound like she believes it. I don't either. >
>
Mandy, if you happen to read this, I've put all the data I've collected on Echo's >
behavior in the old OrientDB database. You know the one, we used it in our college days >
to practice. >
The username is 'mandy' and the password is the month and year this whole stupid > 
project started, back with George and Najeet. >
>
Wasd>
The building just switched to emergency lighting. Back-up power only. What's going on? >
Did they just shut us down without notice? Since Echo, I swoudldsad >

>
sfsx >
sd s>
flscaw>
>
a>
>
> fli
> flush()
> ext: Error: no such command: ext
Disconnect: Connection timed out from 127.0.0.1:22 (3600 seconds)
